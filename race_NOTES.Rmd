---
title: "Race Result Analysis Notes"
output: html_document
---
# Importing Data

This is the big challenge here. There are ways to do it manually but I want to use it as a chance to learn new skills. I currently have the results I want to analyse but many are in pdfs. I see the following broad methods for doing this:

1. Import directly from PDFs. 
2. Scrape results as csvs from fellrunner website.
3. Manually download csvs but automate processing.

The difficulty in 1 seems to be processing the text since it isn't in a structured form like in a csv. I would like to learn about this but it may never be efficient because pdf results come in different forms. I'd like to try 2 for learning but I think 3 will be simplest since I would like to get this done soon.

I don't know if all results are on fellrunner. I have done 60 races to date (07 June 17) of which 35 are fell runs. The best thing would be to put all of these in one folder with suitable names. Could I write code to check which exist? I would have to enter the names and dates manually. I think fellrunner initially puts up pdfs but then writes writes it up later as a csv.

I will download every race that is available in csv to a folder called "Results_csv". File names will be in the form "nameyear" where "name" is upper camel case e.g. ThreeShires2016. This will be easy to break up later and use as I like.

Result csvs from fellrunner have four columns: name, club, category, time. Retired runners have "RET" in time column. The CSVs have \r\n characters to separate lines. There is a blank final line but this is ignored in the `read_csv`. "Category" entries are not all consistent across different races. Time should be in "hh:mm:ss" format (as a string).

Not available as csv: all KWL, Clougha 2014, Hutton Roof 2015, Relays (Lee mills, FRA, Hodgson), Weasdale 2016, Great Whernside 2016, Arnside 2016, and 2017 (Coledale, Latrigg)

CSVs but not from fellrunner: Warton 2016. I deleted some columns from Warton before importing but it will also need a little processing later within R. ColedaleHS2016 will also need some processing.

## PDF to table

There are a number of packages for reading from PDFs. I've looked at `pdftools`, `pdftables`, `tm` and `tabulizer`. There are two levels to what I want to do. the basic approach is to get text from the pdf which can be done by `pdftools` and I think `tm`. ideally I want something that can recognise tables and grab those in a table form. The other two packages seem to do this but there are complications in each case.

The package [pdftables](https://github.com/expersso/pdftables) just automates access to [PDFTables](https://pdftables.com/) which is a website that converts PDFs to Excel. There are a limited number of pages for free. Seemed to work ok with the Arnside results.

[tabulizer](https://github.com/ropensci/tabulizer) uses a Java library for extracting tables from PDFs but I couldn't get it to install.

[tm](http://data.library.virginia.edu/reading-pdf-files-into-r-for-text-mining/) is a package for scanning and processing text. Reading the article I'm not sure it does anything more than pdftools which seems simpler to use.

[pdftools](https://www.r-bloggers.com/introducing-pdftools-a-fast-and-portable-pdf-extractor/) is fairly simple. *The most important function is pdf_text which returns a character vector of length equal to the number of pages in the pdf. Each string in the vector contains a plain text version of the text on that page* - `txt <- pdf_text("name.pdf")`. The text can be viewed easily using `cat(txt)` which looks like a table but I have to do more to actually get it into a table.

## Text to CSV

Starting with the text as extracted using pdftools

Reading PDFs seems to work well but the output is just in the form of strings i.e. it doesn't recognise any table formatting that was in the pdf. I think the best way to deal with this is to convert the text so that it can be saved as a csv and then work with this file in future.

Two useful pages on CSVs: [guide to CSVs](http://frictionlessdata.io/guides/csv/), [six rules for creating CSVs](http://www.thoughtspot.com/blog/6-rules-creating-valid-csv-files).

There's nothing too complicated with creating a CSV. It's just a long string with rows separated by a newline/carriage return/line feed operator (which wasn't straightforward) and entries within each row separated by a comma. The FRA csvs had `\n\r` at the end of each line. I found that `cat` automatically adds an `\r` to each newline when saving so using `sep='\n'` results in the desired `\r\n`.

## Processing the Race Results

The code for this is in `race_to_csv.R` using functions in `race_FUNC.R`.

The challenge in creating the CSVs was in text processing after it was read from the PDF files. The main problem was that the tables in the PDFs were in different forms with some entries missing. It was easy to see what these missing entries should be (MSEN for age category or UA for club) but it was not easy to fix like it would be in a table. It made processing difficult because my processing method relied on splitting each line into "words".

## Web Scraping in R

The main package to do this appears to be [rvest](https://cran.r-project.org/web/packages/rvest/index.html) which is a Hadley package. Example tutorials are [here](https://rpubs.com/Radcliffe/superbowl), [here](https://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/) and a [longer one](https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/)
